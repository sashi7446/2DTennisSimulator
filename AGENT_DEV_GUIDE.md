# Agent Development Guide (LLM-Optimized)

2DTennisSimulator ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹ç™ºã‚¬ã‚¤ãƒ‰ã€‚ã“ã‚Œ1æšã§å®Ÿè£…å¯èƒ½ã€‚

---

## Court Layout

```
      0                    field_width (800)
    0 +------------------------------------------+
      |                    |                     |
      |   [A]              |              [B]    |
      |  id=0          +---+---+          id=1   |
      | (å·¦å´)         |in |in |        (å³å´)   |
      |  agent-a       |_A_|_B_|        agent-b  |
      |                    |                     |
  400 +------------------------------------------+
         â† Aå´ã®å£    ä¸­å¤®(ã‚µãƒ¼ãƒ–åœ°ç‚¹)    Bå´ã®å£ â†’
```

> **è§£èª¬**: æ¨ªé•·ã‚³ãƒ¼ãƒˆã€‚å·¦ã«Aã€å³ã«Bã€‚ä¸­å¤®ã«2ã¤ã®ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼ˆin_A, in_Bï¼‰ãŒã‚ã‚‹ã€‚

---

## Game Flow (å®Ÿæ³ä¸­ç¶™é¢¨)

**ğŸ™ï¸ å®Ÿæ³**: ã€Œã•ã‚ã€ãƒã‚¤ãƒ³ãƒˆé–‹å§‹ã§ã™ï¼ã€

1. **ã‚µãƒ¼ãƒ–** â€” ãƒœãƒ¼ãƒ«ãŒã‚³ãƒ¼ãƒˆä¸­å¤®ã‹ã‚‰ç™ºå°„ã•ã‚Œã¾ã™ã€‚å·¦å³ã©ã¡ã‚‰ã«é£›ã¶ã‹ã¯ãƒ©ãƒ³ãƒ€ãƒ ã€‚
   > **è§£èª¬**: ä»Šå›ã¯å·¦ã«é£›ã³ã¾ã—ãŸã€‚Aå´ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆid=0, agent-aï¼‰ãŒãƒ¬ã‚·ãƒ¼ãƒ–ã§ã™ã€‚

2. **æ‰“çƒ** â€” ãƒœãƒ¼ãƒ«ãŒãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å½“ãŸã‚Šåˆ¤å®šï¼ˆåŠå¾„æ•°ãƒ”ã‚¯ã‚»ãƒ«ã®å††ï¼‰ã«è§¦ã‚Œã‚‹ã¨ã€è‡ªå‹•ã§æ‰“ã¡è¿”ã—ã¾ã™ã€‚æ‰“çƒè§’åº¦ã¯ `action[1]` ã§æŒ‡å®šã—ãŸ `hit_angle` ã®æ–¹å‘ã€‚
   > **è§£èª¬**: ã“ã®ã¨ã `is_in` ãŒ `False` ã«ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚ã‚¢ã‚¦ãƒˆã«ãªã‚‹ã‹ã©ã†ã‹ã€ã¾ã ã‚ã‹ã‚Šã¾ã›ã‚“ã€‚

3. **ã‚¤ãƒ³ã‚¨ãƒªã‚¢é€šé** â€” ãƒœãƒ¼ãƒ«ãŒç›¸æ‰‹å´ã®ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã‚’é€šéï¼
   > **è§£èª¬**: ã€Œ`is_in=True` ã«ãªã‚Šã¾ã—ãŸï¼ã“ã‚Œã§ã“ã®ãƒœãƒ¼ãƒ«ãŒBå´ã®å£ã«åˆ°é”ã™ã‚Œã°ã€Aã®å¾—ç‚¹ã§ã™ã€

4. **å£åˆ°é” â†’ ãƒã‚¤ãƒ³ãƒˆçµ‚äº†**
   - `is_in=True` ã§å£åˆ°é” â†’ **æ‰“ã£ãŸå´ãŒå¾—ç‚¹ï¼** ğŸ‰
   - `is_in=False` ã§å£åˆ°é” â†’ **æ‰“ã£ãŸå´ãŒå¤±ç‚¹ï¼ˆã‚¢ã‚¦ãƒˆï¼‰** ğŸ’€

**ğŸ™ï¸ å®Ÿæ³**: ã€Œãƒœãƒ¼ãƒ«ãŒBå´ã®å£ã«åˆ°é”ï¼is_in ã¯ Trueï¼Aé¸æ‰‹ã®å¾—ç‚¹ã§ã™ï¼ã€

> **è§£èª¬ã¾ã¨ã‚**:
> - 1ãƒã‚¤ãƒ³ãƒˆ = 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
> - æ‰“ã£ãŸã‚‰ `is_in=False`ã€ç›¸æ‰‹ã®ã‚¤ãƒ³ã‚¨ãƒªã‚¢é€šéã§ `is_in=True`
> - å£åˆ°é”æ™‚ã® `is_in` ã§å‹æ•—ãŒæ±ºã¾ã‚‹

---

## Observation (dict)

| Key | Type | Description |
|-----|------|-------------|
| `ball_x`, `ball_y` | float | ãƒœãƒ¼ãƒ«åº§æ¨™ |
| `ball_vx`, `ball_vy` | float | ãƒœãƒ¼ãƒ«é€Ÿåº¦ |
| `ball_is_in` | bool | ã‚¤ãƒ³ã‚¨ãƒªã‚¢é€šéæ¸ˆã¿ã‹ |
| `player_a_x`, `player_a_y` | float | ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Aåº§æ¨™ |
| `player_b_x`, `player_b_y` | float | ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Båº§æ¨™ |
| `score_a`, `score_b` | int | ã‚¹ã‚³ã‚¢ |
| `rally_count` | int | ãƒ©ãƒªãƒ¼å›æ•° |
| `field_width`, `field_height` | int | ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚º |

---

## Action (tuple[int, float])

```python
(movement, hit_angle)
```

- `movement`: 0-15 = 22.5Â°åˆ»ã¿16æ–¹å‘, 16 = é™æ­¢
- `hit_angle`: 0-360Â° (0=å³, 90=ä¸‹, 180=å·¦, 270=ä¸Š)

---

## Minimal Implementation

```python
# agents/my_agent.py
from agents.base import Agent, AgentConfig

class MyAgent(Agent):
    def __init__(self):
        super().__init__(AgentConfig(
            name="MyAgent",
            agent_type="my_agent",
            description="My custom agent"
        ))

    def act(self, obs: dict) -> tuple[int, float]:
        # Example: chase ball
        my_x = obs["player_a_x"] if self.player_id == 0 else obs["player_b_x"]
        my_y = obs["player_a_y"] if self.player_id == 0 else obs["player_b_y"]
        dx, dy = obs["ball_x"] - my_x, obs["ball_y"] - my_y

        import math
        angle = math.degrees(math.atan2(dy, dx)) % 360
        movement = int(angle / 22.5) % 16

        # hit toward opponent's side
        hit_angle = 180 if self.player_id == 0 else 0
        return (movement, hit_angle)

    def learn(self, reward: float, done: bool) -> None:
        pass  # Optional: implement learning
```

---

## Registration (3 steps)

### 1. `agents/__init__.py`
```python
from agents.my_agent import MyAgent
__all__ = [..., "MyAgent"]
```

### 2. `agents/base.py` ã® `get_agent_class()`
```python
classes = {
    ...,
    "my_agent": MyAgent,
}
```

### 3. Run
```bash
python main.py --agent-a my_agent --agent-b chase
```

---

## Tips

- `self.player_id`: 0=A(å·¦å´), 1=B(å³å´) - `set_player_id()` ã§è‡ªå‹•è¨­å®š
- æ‰“æ’ƒåˆ¤å®šã¯ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ãƒœãƒ¼ãƒ«ã®è·é›¢ã§è‡ªå‹•åˆ¤å®š
- `learn()` ã¯æ¯ã‚¹ãƒ†ãƒƒãƒ—å‘¼ã°ã‚Œã‚‹ï¼ˆreward, done ã‚’å—ã‘å–ã‚‹ï¼‰
- `reset()` ã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚ã«å‘¼ã°ã‚Œã‚‹

---

## Test

```bash
python -m pytest tests/test_agents.py -v -k "MyAgent"
```
